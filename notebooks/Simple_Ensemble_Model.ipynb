{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import os\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44400cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths (same as original code)\n",
    "BASE_PATH = '/workspace/COMP-3608---PROJECT'\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
    "MODELS_PATH = os.path.join(BASE_PATH, 'models')\n",
    "RESULTS_PATH = os.path.join(BASE_PATH, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, f\"{title.replace(' ', '_')}.png\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, roc_auc, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, f\"{title.replace(' ', '_')}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Print and return metrics for a model's predictions\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_voting_classifier(model_dict, X_test, y_test, dataset_name, voting='hard'):\n",
    "\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Creating Ensemble for {dataset_name} dataset\")\n",
    "    \n",
    "    model_predictions = {}\n",
    "    model_probabilities = {}\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for name, model in model_dict.items():\n",
    "        model_predictions[name] = model.predict(X_test)\n",
    "        model_probabilities[name] = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    if voting == 'hard':\n",
    "        # Majority voting (hard voting)\n",
    "        ensemble_pred = np.zeros(len(y_test))\n",
    "        for name in model_predictions:\n",
    "            ensemble_pred += model_predictions[name]\n",
    "        \n",
    "        # If more than half of the models predicted 1, set to 1\n",
    "        ensemble_pred = (ensemble_pred >= len(model_dict)/2).astype(int)\n",
    "    \n",
    "    elif voting == 'soft':\n",
    "        # Averaging probabilities (soft voting)\n",
    "        ensemble_prob = np.zeros(len(y_test))\n",
    "        for name in model_probabilities:\n",
    "            ensemble_prob += model_probabilities[name]\n",
    "        \n",
    "        ensemble_prob /= len(model_probabilities)  # average probabilities\n",
    "        ensemble_pred = (ensemble_prob >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate ensemble metrics\n",
    "    metrics = print_metrics(y_test, ensemble_pred, f\"Ensemble ({voting} voting)\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(\n",
    "        metrics['confusion_matrix'], \n",
    "        ['Negative', 'Positive'], \n",
    "        f'Confusion Matrix - Ensemble {voting.capitalize()} Voting ({dataset_name})'\n",
    "    )\n",
    "    \n",
    "    # Calculate and plot ROC curve (for soft voting)\n",
    "    if voting == 'soft':\n",
    "        fpr, tpr, _ = roc_curve(y_test, ensemble_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plot_roc_curve(\n",
    "            fpr, tpr, roc_auc, \n",
    "            f'ROC Curve - Ensemble {voting.capitalize()} Voting ({dataset_name})'\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'predictions': ensemble_pred,\n",
    "        'probabilities': ensemble_prob if voting == 'soft' else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568baa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_base_models(ensemble_results, base_model_results, dataset_name):\n",
    "    \"\"\"Compare ensemble model with base models\"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    # Extract metrics from base models\n",
    "    base_metrics = {\n",
    "        model_name: {metric: base_model_results[model_name][metric] for metric in metrics}\n",
    "        for model_name in base_model_results\n",
    "    }\n",
    "    \n",
    "    # Add ensemble metrics\n",
    "    all_metrics = {\n",
    "        **base_metrics,\n",
    "        'Ensemble (hard)': {metric: ensemble_results['hard']['metrics'][metric] for metric in metrics},\n",
    "        'Ensemble (soft)': {metric: ensemble_results['soft']['metrics'][metric] for metric in metrics}\n",
    "    }\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_data = []\n",
    "    for model_name, model_metrics in all_metrics.items():\n",
    "        for metric_name, metric_value in model_metrics.items():\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Metric': metric_name,\n",
    "                'Value': metric_value\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        metric_data = comparison_df[comparison_df['Metric'] == metric]\n",
    "        \n",
    "        sns.barplot(x='Model', y='Value', data=metric_data)\n",
    "        plt.title(f'{metric.capitalize()} Comparison for {dataset_name}')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.suptitle(f'Model Performance Comparison for {dataset_name}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, f\"{dataset_name}_model_comparison.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
