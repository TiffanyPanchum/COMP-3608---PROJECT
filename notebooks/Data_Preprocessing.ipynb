{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m RAW_DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m PROCESSED_DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(PROCESSED_DATA_PATH, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "RAW_DATA_PATH = 'data/raw'\n",
    "PROCESSED_DATA_PATH = 'data/processed'\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    \"\"\"Load dataset from raw folder\"\"\"\n",
    "    filepath = os.path.join(RAW_DATA_PATH, filename)\n",
    "    if filename.endswith('.csv'):\n",
    "        return pd.read_csv(filepath)\n",
    "    elif filename.endswith('.xlsx'):\n",
    "        return pd.read_csv(filepath)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(df, filename):\n",
    "    \"\"\"Save cleaned dataset to processed folder\"\"\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_PATH, filename)\n",
    "    df.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_diabetes_data(df):\n",
    "    \"\"\"Custom cleaning for diabetes dataset based on EDA findings\"\"\"\n",
    "    # Convert age from float to int (ages should be whole numbers)\n",
    "    df['age'] = df['age'].astype(int)\n",
    "    \n",
    "    # Clean smoking_history categories\n",
    "    df['smoking_history'] = df['smoking_history'].replace({\n",
    "        'No Info': 'Unknown',\n",
    "        'never': 'Never',\n",
    "        'current': 'Current',\n",
    "        'former': 'Former',\n",
    "        'not current': 'Not current',\n",
    "        'ever': 'Ever'\n",
    "    })\n",
    "    \n",
    "    # Handle potential outliers in BMI (clip extreme values)\n",
    "    df['bmi'] = df['bmi'].clip(lower=15, upper=50)\n",
    "    \n",
    "    # Create age groups feature\n",
    "    bins = [0, 18, 30, 45, 60, 80]\n",
    "    labels = ['0-18', '19-30', '31-45', '46-60', '60+']\n",
    "    df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels)\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stroke_data(df):\n",
    "    \"\"\"Custom cleaning for stroke dataset based on EDA findings\"\"\"\n",
    "    # Drop ID column as it's not useful for modeling\n",
    "    df = df.drop('id', axis=1)\n",
    "    \n",
    "    # Handle missing BMI values (201 missing)\n",
    "    # Impute with median by age group\n",
    "    df['age_group'] = pd.cut(df['age'], \n",
    "                            bins=[0, 18, 30, 45, 60, 100],\n",
    "                            labels=['0-18', '19-30', '31-45', '46-60', '60+'])\n",
    "    df['bmi'] = df.groupby('age_group')['bmi'].apply(\n",
    "        lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # Clean smoking_status categories\n",
    "    df['smoking_status'] = df['smoking_status'].replace({\n",
    "        'formerly smoked': 'Former',\n",
    "        'never smoked': 'Never',\n",
    "        'smokes': 'Current',\n",
    "        'Unknown': 'Unknown'\n",
    "    })\n",
    "    \n",
    "    # Convert categorical variables to lowercase\n",
    "    categorical_cols = ['gender', 'ever_married', 'work_type', \n",
    "                       'residence_type', 'smoking_status']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].str.lower()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_heart_data(df):\n",
    "    \"\"\"Custom cleaning for heart disease dataset based on EDA findings\"\"\"\n",
    "    # Drop columns with excessive missing values\n",
    "    df = df.drop(['slope', 'ca', 'thal'], axis=1)\n",
    "    \n",
    "    # Handle missing values in other columns\n",
    "    # For numerical columns, impute with median\n",
    "    num_cols = ['trestbps', 'chol', 'thalch', 'oldpeak']\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # For categorical columns, impute with mode\n",
    "    cat_cols = ['fbs', 'restecg', 'exang']\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # Clean categorical variables\n",
    "    df['cp'] = df['cp'].str.replace('-', ' ').str.lower()\n",
    "    df['restecg'] = df['restecg'].str.lower()\n",
    "    df['exang'] = df['exang'].astype(str).str.lower()\n",
    "    \n",
    "    # Handle biological impossibilities (0 values for cholesterol and blood pressure)\n",
    "    df['chol'] = df['chol'].replace(0, df['chol'].median())\n",
    "    df['trestbps'] = df['trestbps'].replace(0, df['trestbps'].median())\n",
    "    \n",
    "    # Create age groups\n",
    "    bins = [0, 30, 40, 50, 60, 70, 100]\n",
    "    labels = ['<30', '30-39', '40-49', '50-59', '60-69', '70+']\n",
    "    df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Diabetes Dataset\n",
    "diabetes_df = load_dataset('diabetes_prediction_dataset.csv')\n",
    "diabetes_clean = clean_diabetes_data(diabetes_df)\n",
    "save_processed_data(diabetes_clean, 'processed_diabetes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Heart Disease Dataset\n",
    "heart_df = load_dataset('heart_disease_uci.csv')\n",
    "heart_clean = clean_heart_data(heart_df)\n",
    "save_processed_data(heart_clean, 'processed_heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Stroke Dataset\n",
    "stroke_df = load_dataset('healthcare-dataset-stroke-data.csv')\n",
    "stroke_clean = clean_stroke_data(stroke_df)\n",
    "save_processed_data(stroke_clean, 'processed_stroke.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
